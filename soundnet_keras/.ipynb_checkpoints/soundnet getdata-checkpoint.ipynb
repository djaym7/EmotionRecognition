{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization, Activation, Conv1D, MaxPooling1D, ZeroPadding1D, InputLayer\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa #audio library\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Builds up the SoundNet model and loads the weights from a given model file (8-layer model is stored at models/sound8.npy).\n",
    "    :return: The model built according to architecture and weights pre-stablished      \n",
    "    \"\"\"\n",
    "    model_weights = np.load(r\"C:\\Users\\djaym7\\Desktop\\Github\\EmotionRecognition\\soundnet_keras\\models\\sound8.npy\",encoding = 'latin1').item()\n",
    "    model = Sequential()\n",
    "    #Input layer: audio raw waveform (1,length_audio,1)\n",
    "    model.add(InputLayer(batch_input_shape=(1, None, 1)))\n",
    "\n",
    "    filter_parameters = [{'name': 'conv1', 'num_filters': 16, 'padding': 32,\n",
    "                          'kernel_size': 64, 'conv_strides': 2,\n",
    "                          'pool_size': 8, 'pool_strides': 8}, #pool1\n",
    "\n",
    "                         {'name': 'conv2', 'num_filters': 32, 'padding': 16,\n",
    "                          'kernel_size': 32, 'conv_strides': 2,\n",
    "                          'pool_size': 8, 'pool_strides': 8}, #pool2\n",
    "\n",
    "                         {'name': 'conv3', 'num_filters': 64, 'padding': 8,\n",
    "                          'kernel_size': 16, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv4', 'num_filters': 128, 'padding': 4,\n",
    "                          'kernel_size': 8, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv5', 'num_filters': 256, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2,\n",
    "                          'pool_size': 4, 'pool_strides': 4}, #pool5\n",
    "\n",
    "                         {'name': 'conv6', 'num_filters': 512, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv7', 'num_filters': 1024, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv8_2', 'num_filters': 401, 'padding': 0,\n",
    "                          'kernel_size': 8, 'conv_strides': 2},#output: VGG 401 classes\n",
    "                         ]\n",
    "\n",
    "    for x in filter_parameters:\n",
    "        #for each [zero_padding - conv - batchNormalization - relu]\n",
    "        model.add(ZeroPadding1D(padding=x['padding']))\n",
    "        model.add(Conv1D(x['num_filters'],\n",
    "                         kernel_size=x['kernel_size'],\n",
    "                         strides=x['conv_strides'],\n",
    "                         padding='valid'))\n",
    "        weights = model_weights[x['name']]['weights'].reshape(model.layers[-1].get_weights()[0].shape)\n",
    "        biases = model_weights[x['name']]['biases']\n",
    "\n",
    "        model.layers[-1].set_weights([weights, biases])  #set weights in convolutional layer\n",
    "\n",
    "        if 'conv8' not in x['name']:\n",
    "            gamma = model_weights[x['name']]['gamma']\n",
    "            beta = model_weights[x['name']]['beta']\n",
    "            mean = model_weights[x['name']]['mean']\n",
    "            var = model_weights[x['name']]['var']\n",
    "\n",
    "            \n",
    "            model.add(BatchNormalization())\n",
    "            model.layers[-1].set_weights([gamma, beta, mean, var]) #set weights in batchNormalization\n",
    "            model.add(Activation('relu'))\n",
    "            \n",
    "        if 'pool_size' in x:\n",
    "            #add 3 pooling layers\n",
    "            model.add(MaxPooling1D(pool_size=x['pool_size'],\n",
    "                                   strides=x['pool_strides'],\n",
    "                                   padding='valid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-297265a59c5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'soundnet.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('soundnet.pickle',mode='wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(X, window_size, window_step):\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw,ws),dtype = a.dtype)\n",
    "\n",
    "    for i in range(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start : stop]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(audio):\n",
    "    audio *= 256.0  # SoundNet requires an input range between -256 and 256\n",
    "    # reshaping the audio data, in this way it fits into the graph (batch_size, num_samples, num_filter_channels)\n",
    "    audio = np.reshape(audio, (1, -1, 1))\n",
    "    return audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "model = build_model()\n",
    "\n",
    "# return the list of activations as tensors for an specific layer in the model \n",
    "def getActivations(data,number_layer):\n",
    "    intermediate_tensor = []\n",
    "    #get Hidden Representation function\n",
    "    get_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[number_layer].output])\n",
    "    \n",
    "    for audio in data:\n",
    "        #get Hidden Representation       \n",
    "        layer_output = get_layer_output([audio])[0] # multidimensional vector\n",
    "        tensor = layer_output.reshape(1,-1) # change vector shape to 1 (tensor)\n",
    "        intermediate_tensor.append(tensor[0]) # list of tensor activations for each object in Esc10\n",
    "    return intermediate_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_file):\n",
    "    sample_rate = 22050  # SoundNet works on monophonic-audio files with sample rate of 22050.\n",
    "    audio, sr = librosa.load(audio_file, dtype='float32', sr=sample_rate, mono=True) #load audio\n",
    "    max_size = 110361\n",
    "    audio=np.pad(audio,pad_width=(0,max_size-len(audio)),mode='constant')\n",
    "    \n",
    "    #overlapping \n",
    "    audio = overlap(audio,20,10)\n",
    "    output = []\n",
    "    for i in audio[:10]:\n",
    "        i = preprocess(i)\n",
    "        i = np.asarray(getActivations([i],31))\n",
    "        output.append(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #audio = preprocess(audio)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = load_audio(r\"C:\\Users\\djaym7\\Desktop\\Github\\EmotionRecognition\\soundnet_keras\\kk.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
