{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply,Dropout\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#address to getData->output folder\n",
    "address = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(id):\n",
    "    with open(address + f'y_train_{id}','rb') as f:\n",
    "        y_train=np.array(pickle.load(f))\n",
    "    with open(address+f'y_test_{id}','rb') as f:\n",
    "        y_test=np.array(pickle.load(f))\n",
    "    with open(address+ f'X_train_{id}','rb') as f:\n",
    "        X_train = np.array(pickle.load(f))\n",
    "    with open(address+f'X_test_{id}','rb') as f:\n",
    "        X_test = np.array(pickle.load(f))\n",
    "\n",
    "    scalers = {}\n",
    "    for k in range(X_train.shape[2]):\n",
    "        scalers[k] = StandardScaler()\n",
    "        X_train[:, k, :] = scalers[k].fit_transform(X_train[:, k, :]) \n",
    "\n",
    "    for j in range(X_test.shape[2]):\n",
    "        X_test[:, j, :] = scalers[j].transform(X_test[:, j, :])\n",
    "    \n",
    "    print(f'id : {id}')\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "   \n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1002\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500, 13)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 500, 16)      1984        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 500, 96)      0           s0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 112)     0           dropout_1[0][0]                  \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500, 1)       113         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 500, 1)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 16)        0           attention_weights[0][0]          \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 96), (None,  43776       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            582         cu_dnnlstm_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 46,455\n",
      "Trainable params: 46,455\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "id : 1002\n",
      "Train on 7361 samples, validate on 81 samples\n",
      "Epoch 1/60\n",
      "1280/7361 [====>.........................] - ETA: 38s - loss: 1.8228 - acc: 0.1758"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e1ad47951396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mfilePath\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34mf'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\ExtendedSavedModels\\\\{i}.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms0_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc0_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_num_people=[]\n",
    "for i in range(1,92):\n",
    "    if i <10:\n",
    "        total_num_people.append('100'+str(i))\n",
    "    else:\n",
    "        total_num_people.append('10'+str(i))\n",
    "\n",
    "for i in total_num_people:\n",
    "    if int(i)>1001:\n",
    "        X_train=[]\n",
    "        X_test=[]\n",
    "        y_train=[]\n",
    "        y_test=[]\n",
    "        X_train,y_train,X_test,y_test=loadData(i)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "    #load data here\n",
    "    #input_shape = batch_size,time_steps,input_dim\n",
    "\n",
    "    #batch_size = X_train.shape[0]   #automatically taken no need to specify\n",
    "    time_steps = X_train.shape[1]\n",
    "    input_dim = X_train.shape[2]\n",
    "\n",
    "\n",
    "\n",
    "    hid_pre_acts = [16]\n",
    "    hid_post_acts = [96]\n",
    "    #dense1s = [16,32,64,96,128,256]\n",
    "    #dense2s = [16,32,64,96,128,256]\n",
    "    for hid_pre_act in hid_pre_acts:\n",
    "        for hid_post_act in hid_post_acts:\n",
    "            # Defined shared layers as global variables\n",
    "            repeator = RepeatVector(time_steps) #500\n",
    "            concatenator = Concatenate(axis=-1)\n",
    "            densor = Dense(1, activation = \"relu\")\n",
    "            activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "            dotor = Dot(axes = 1)\n",
    "\n",
    "            #one step attention\n",
    "\n",
    "            def one_step_attention(a,s_prev):\n",
    "\n",
    "                s_prev = repeator(s_prev)\n",
    "                concat = concatenator([a,s_prev])\n",
    "                e = densor(concat)\n",
    "                alphas = activator(e)\n",
    "                context = dotor([alphas, a])\n",
    "\n",
    "                return context\n",
    "            \n",
    "            post_activation_lstm_cell = CuDNNLSTM(hid_post_act,return_state=True)\n",
    "            #output_layer = Dense(dense1,activation='relu')\n",
    "            #onemore = Dense(dense2,activation='relu')\n",
    "            fin_out = Dense(6,activation='softmax')\n",
    "\n",
    "                # model\n",
    "\n",
    "            def model(time_steps,input_dim,hid_post_act,hid_pre_act):\n",
    "\n",
    "                X = Input(shape =(time_steps,input_dim))\n",
    "                s0 = Input(shape=(hid_post_act,),name='s0')\n",
    "                c0 = Input(shape=(hid_post_act,),name='c0')\n",
    "                s=s0\n",
    "                c=c0\n",
    "                outputs=[]\n",
    "\n",
    "\n",
    "\n",
    "                a = CuDNNLSTM(hid_pre_act,return_sequences=True)(X)\n",
    "                a = Dropout(0.4)(a)\n",
    "                #6 is final output size\n",
    "                for t in range(1):\n",
    "                    context =one_step_attention(a,s)\n",
    "                    s, _, c = post_activation_lstm_cell(context, initial_state=[s, c])\n",
    "                    #out = output_layer(s)\n",
    "                    #o = onemore(out)\n",
    "                    k=fin_out(s)\n",
    "                    outputs.append(k)\n",
    "\n",
    "\n",
    "                model = Model([X, s0, c0], outputs)\n",
    "\n",
    "                return model       \n",
    "\n",
    "            model = model(time_steps,input_dim,hid_post_act,hid_pre_act)\n",
    "\n",
    "            opt = Adam()\n",
    "            #opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "            model.summary()\n",
    "            plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "            s0 = np.zeros((X_train.shape[0], hid_post_act))\n",
    "            c0 = np.zeros((X_train.shape[0], hid_post_act))\n",
    "            s0_t = np.zeros((X_test.shape[0], hid_post_act))\n",
    "            c0_t = np.zeros((X_test.shape[0], hid_post_act))\n",
    "            print(f'id : {i}')\n",
    "            filePath =f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\ExtendedSavedModels\\\\{i}.h5'\n",
    "            checkpoint = ModelCheckpoint(filepath=filePath,monitor='val_acc',mode='max',save_best_only=True,verbose=1,save_weights_only=False)\n",
    "            model.fit([X_train, s0, c0], y_train, epochs=60,validation_data=([X_test,s0_t,c0_t],y_test),callbacks=[checkpoint])\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
