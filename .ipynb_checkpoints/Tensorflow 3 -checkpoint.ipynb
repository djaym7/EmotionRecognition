{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply,Dropout\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(id):\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\y_train_{id}','rb') as f:\n",
    "        y_train=np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\y_test_{id}','rb') as f:\n",
    "        y_test=np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\X_train_{id}','rb') as f:\n",
    "        X_train = np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\X_test_{id}','rb') as f:\n",
    "        X_test = np.array(pickle.load(f))\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_train',y_train.shape) \n",
    "    print('y_test',y_test.shape) \n",
    "\n",
    "   \n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7360, 500, 13)\n",
      "X_test: (82, 500, 13)\n",
      "y_train (7360, 6)\n",
      "y_test (82, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test=loadData(1001)\n",
    "#input_shape = batch_size,time_steps,input_dim\n",
    "\n",
    "#batch_size = X_train.shape[0]   #automatically taken no need to specify\n",
    "time_steps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(time_steps) #500\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step attention\n",
    "\n",
    "def one_step_attention(a,s_prev):\n",
    "    \n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    e = densor(concat)\n",
    "    alphas = activator(e)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_pre_act = 32\n",
    "hid_post_act = 64\n",
    "post_activation_lstm_cell = CuDNNLSTM(hid_post_act,return_state=True)\n",
    "output_layer = Dense(128,activation='relu')\n",
    "onemore = Dense(64,activation='relu')\n",
    "fin_out = Dense(6,activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "def model(time_steps,input_dim,hid_post_act,hid_pre_act):\n",
    "       \n",
    "    X = Input(shape =(time_steps,input_dim))\n",
    "    s0 = Input(shape=(hid_post_act,),name='s0')\n",
    "    c0 = Input(shape=(hid_post_act,),name='c0')\n",
    "    s=s0\n",
    "    c=c0\n",
    "    outputs=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    a = CuDNNLSTM(hid_pre_act,return_sequences=True)(X)\n",
    "    a = Dropout(0.4)(a)\n",
    "    #6 is final output size\n",
    "    for t in range(1):\n",
    "        context =one_step_attention(a,s)\n",
    "        s, _, c = post_activation_lstm_cell(context, initial_state=[s, c])\n",
    "        out = output_layer(s)\n",
    "        o = onemore(out)\n",
    "        k=fin_out(o)\n",
    "        outputs.append(k)\n",
    "    \n",
    "        \n",
    "    model = Model([X, s0, c0], outputs)\n",
    "    \n",
    "    return model       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(time_steps,input_dim,hid_post_act,hid_pre_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((7360, hid_post_act))\n",
    "c0 = np.zeros((7360, hid_post_act))\n",
    "s0_t = np.zeros((82, hid_post_act))\n",
    "c0_t = np.zeros((82, hid_post_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7360 samples, validate on 82 samples\n",
      "Epoch 1/90\n",
      "7360/7360 [==============================] - 20s 3ms/step - loss: 1.7176 - acc: 0.2485 - val_loss: 1.7039 - val_acc: 0.2683\n",
      "Epoch 2/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.6074 - acc: 0.3211 - val_loss: 1.6186 - val_acc: 0.2927\n",
      "Epoch 3/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.5630 - acc: 0.3533 - val_loss: 1.6144 - val_acc: 0.3049\n",
      "Epoch 4/90\n",
      "7360/7360 [==============================] - 18s 2ms/step - loss: 1.5306 - acc: 0.3678 - val_loss: 1.6262 - val_acc: 0.3049\n",
      "Epoch 5/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.5123 - acc: 0.3726 - val_loss: 1.6424 - val_acc: 0.3171\n",
      "Epoch 6/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4994 - acc: 0.3800 - val_loss: 1.6399 - val_acc: 0.2683\n",
      "Epoch 7/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4841 - acc: 0.3958 - val_loss: 1.6057 - val_acc: 0.3171\n",
      "Epoch 8/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4761 - acc: 0.3940 - val_loss: 1.5558 - val_acc: 0.4024\n",
      "Epoch 9/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4645 - acc: 0.4019 - val_loss: 1.6258 - val_acc: 0.2927\n",
      "Epoch 10/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4612 - acc: 0.4069 - val_loss: 1.6261 - val_acc: 0.2805\n",
      "Epoch 11/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4518 - acc: 0.4084 - val_loss: 1.6898 - val_acc: 0.3049\n",
      "Epoch 12/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4458 - acc: 0.4144 - val_loss: 1.6459 - val_acc: 0.2927\n",
      "Epoch 13/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4414 - acc: 0.4099 - val_loss: 1.6066 - val_acc: 0.3415\n",
      "Epoch 14/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4425 - acc: 0.4139 - val_loss: 1.6385 - val_acc: 0.2927\n",
      "Epoch 15/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4358 - acc: 0.4220 - val_loss: 1.6019 - val_acc: 0.3415\n",
      "Epoch 16/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4290 - acc: 0.4185 - val_loss: 1.6350 - val_acc: 0.2927\n",
      "Epoch 17/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4316 - acc: 0.4148 - val_loss: 1.6556 - val_acc: 0.2927\n",
      "Epoch 18/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4273 - acc: 0.4196 - val_loss: 1.6341 - val_acc: 0.3293\n",
      "Epoch 19/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4281 - acc: 0.4201 - val_loss: 1.6407 - val_acc: 0.3171\n",
      "Epoch 20/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4265 - acc: 0.4212 - val_loss: 1.6362 - val_acc: 0.3293\n",
      "Epoch 21/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4218 - acc: 0.4213 - val_loss: 1.6652 - val_acc: 0.2927\n",
      "Epoch 22/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4184 - acc: 0.4269 - val_loss: 1.6575 - val_acc: 0.3293\n",
      "Epoch 23/90\n",
      "7360/7360 [==============================] - 18s 2ms/step - loss: 1.4159 - acc: 0.4254 - val_loss: 1.6425 - val_acc: 0.3049\n",
      "Epoch 24/90\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.4175 - acc: 0.4215 - val_loss: 1.6482 - val_acc: 0.2805\n",
      "Epoch 25/90\n",
      "3296/7360 [============>.................] - ETA: 9s - loss: 1.4141 - acc: 0.4132"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, s0, c0], y_train, epochs=90,validation_data=([X_test,s0_t,c0_t],y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
