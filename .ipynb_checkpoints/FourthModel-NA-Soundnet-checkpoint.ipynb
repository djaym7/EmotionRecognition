{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply,Dropout\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model,Sequential\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.vis_utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#address to getData->output\n",
    "addres = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(id):\n",
    "    with open(address + f'y_train_{id}','rb') as f:\n",
    "        y_train=np.array(pickle.load(f))\n",
    "    with open(address+f'y_test_{id}','rb') as f:\n",
    "        y_test=np.array(pickle.load(f))\n",
    "    with open(address+ f'X_train_{id}','rb') as f:\n",
    "        X_train = np.array(pickle.load(f))\n",
    "    with open(address+f'X_test_{id}','rb') as f:\n",
    "        X_test = np.array(pickle.load(f))\n",
    "\n",
    "    scalers = {}\n",
    "    for k in range(X_train.shape[2]):\n",
    "        scalers[k] = StandardScaler()\n",
    "        X_train[:, k, :] = scalers[k].fit_transform(X_train[:, k, :]) \n",
    "\n",
    "    for j in range(X_test.shape[2]):\n",
    "        X_test[:, j, :] = scalers[j].transform(X_test[:, j, :])\n",
    "    \n",
    "    print(f'id : {id}')\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "   \n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1001\n",
      "id : 1001\n",
      "Train on 7360 samples, validate on 82 samples\n",
      "Epoch 1/60\n",
      "7360/7360 [==============================] - 32s 4ms/step - loss: 1.7964 - acc: 0.1709 - val_loss: 1.7958 - val_acc: 0.1707\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.17073, saving model to C:\\Users\\djaym7\\Desktop\\Github\\EmotionRecognition\\ExtendedSavedModels\\NA1001.h5\n",
      "Epoch 2/60\n",
      "2528/7360 [=========>....................] - ETA: 19s - loss: 1.7938 - acc: 0.1697"
     ]
    }
   ],
   "source": [
    "\n",
    "total_num_people=[]\n",
    "for i in range(1,92):\n",
    "    if i <10:\n",
    "        total_num_people.append('100'+str(i))\n",
    "    else:\n",
    "        total_num_people.append('10'+str(i))\n",
    "\n",
    "for i in total_num_people:\n",
    "    if True:\n",
    "        X_train=[]\n",
    "        X_test=[]\n",
    "        y_train=[]\n",
    "        y_test=[]\n",
    "        X_train,y_train,X_test,y_test=loadData(i)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "\n",
    "\n",
    "    #load data here\n",
    "    #input_shape = batch_size,time_steps,input_dim\n",
    "\n",
    "    #batch_size = X_train.shape[0]   #automatically taken no need to specify\n",
    "    time_steps = X_train.shape[1]\n",
    "    input_dim = X_train.shape[2]\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    model.add(CuDNNLSTM(16,input_shape=(time_steps,input_dim),return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(CuDNNLSTM(96))\n",
    "    model.add(Dense(6,activation='softmax'))\n",
    " \n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    print(f'id : {i}')\n",
    "    filePath =f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\ExtendedSavedModels\\\\NA{i}.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath=filePath,monitor='val_acc',mode='max',save_best_only=True,verbose=1,save_weights_only=False)\n",
    "            \n",
    "    model.fit(X_train, y_train, epochs=60,validation_data=(X_test,y_test),callbacks=[checkpoint])\n",
    "    #model.save(f'NA{i}.h5')\n",
    "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
