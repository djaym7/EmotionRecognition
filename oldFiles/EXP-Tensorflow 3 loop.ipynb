{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply,Dropout\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(id):\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\y_train_{id}','rb') as f:\n",
    "        y_train=np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\y_test_{id}','rb') as f:\n",
    "        y_test=np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\X_train_{id}','rb') as f:\n",
    "        X_train = np.array(pickle.load(f))\n",
    "    with open(f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\data 02\\\\X_test_{id}','rb') as f:\n",
    "        X_test = np.array(pickle.load(f))\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_train',y_train.shape) \n",
    "    print('y_test',y_test.shape) \n",
    "\n",
    "   \n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7360, 500, 13)\n",
      "X_test: (82, 500, 13)\n",
      "y_train (7360, 6)\n",
      "y_test (82, 6)\n",
      "FINAL-[16,96] id -1011---1544420670\n",
      "id : 1011\n",
      "Train on 7360 samples, validate on 82 samples\n",
      "Epoch 1/150\n",
      "7360/7360 [==============================] - 17s 2ms/step - loss: 1.6955 - acc: 0.2870 - val_loss: 1.5407 - val_acc: 0.3659\n",
      "Epoch 2/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.5443 - acc: 0.3694 - val_loss: 1.4162 - val_acc: 0.3902\n",
      "Epoch 3/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4931 - acc: 0.3886 - val_loss: 1.3748 - val_acc: 0.4390\n",
      "Epoch 4/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4651 - acc: 0.4043 - val_loss: 1.3725 - val_acc: 0.4878\n",
      "Epoch 5/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4409 - acc: 0.4149 - val_loss: 1.3395 - val_acc: 0.4512\n",
      "Epoch 6/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4305 - acc: 0.4242 - val_loss: 1.3380 - val_acc: 0.4756\n",
      "Epoch 7/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4197 - acc: 0.4247 - val_loss: 1.3113 - val_acc: 0.4756\n",
      "Epoch 8/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4058 - acc: 0.4329 - val_loss: 1.3291 - val_acc: 0.4756\n",
      "Epoch 9/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.4054 - acc: 0.4284 - val_loss: 1.2759 - val_acc: 0.4756\n",
      "Epoch 10/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3902 - acc: 0.4326 - val_loss: 1.2922 - val_acc: 0.4878\n",
      "Epoch 11/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3812 - acc: 0.4398 - val_loss: 1.4504 - val_acc: 0.4390\n",
      "Epoch 12/150\n",
      "7360/7360 [==============================] - 14s 2ms/step - loss: 1.4398 - acc: 0.4200 - val_loss: 1.3592 - val_acc: 0.4390\n",
      "Epoch 13/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3910 - acc: 0.4408 - val_loss: 1.3147 - val_acc: 0.4756\n",
      "Epoch 14/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3641 - acc: 0.4489 - val_loss: 1.3239 - val_acc: 0.4634\n",
      "Epoch 15/150\n",
      "7360/7360 [==============================] - 14s 2ms/step - loss: 1.3545 - acc: 0.4586 - val_loss: 1.3159 - val_acc: 0.4390\n",
      "Epoch 16/150\n",
      "7360/7360 [==============================] - 14s 2ms/step - loss: 1.3512 - acc: 0.4572 - val_loss: 1.2659 - val_acc: 0.5000\n",
      "Epoch 17/150\n",
      "7360/7360 [==============================] - 14s 2ms/step - loss: 1.3344 - acc: 0.4707 - val_loss: 1.2701 - val_acc: 0.5122\n",
      "Epoch 18/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3366 - acc: 0.4656 - val_loss: 1.3324 - val_acc: 0.5000\n",
      "Epoch 19/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3278 - acc: 0.4677 - val_loss: 1.2615 - val_acc: 0.5000\n",
      "Epoch 20/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3142 - acc: 0.4765 - val_loss: 1.3547 - val_acc: 0.4756\n",
      "Epoch 21/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3100 - acc: 0.4746 - val_loss: 1.3496 - val_acc: 0.4634\n",
      "Epoch 22/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.3037 - acc: 0.4758 - val_loss: 1.3576 - val_acc: 0.4756\n",
      "Epoch 23/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2960 - acc: 0.4755 - val_loss: 1.3234 - val_acc: 0.5488\n",
      "Epoch 24/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2860 - acc: 0.4808 - val_loss: 1.2961 - val_acc: 0.5000\n",
      "Epoch 25/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2831 - acc: 0.4844 - val_loss: 1.3494 - val_acc: 0.4268\n",
      "Epoch 26/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2751 - acc: 0.4852 - val_loss: 1.3457 - val_acc: 0.4756\n",
      "Epoch 27/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2679 - acc: 0.4962 - val_loss: 1.3398 - val_acc: 0.4756\n",
      "Epoch 28/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2662 - acc: 0.4944 - val_loss: 1.2892 - val_acc: 0.4878\n",
      "Epoch 29/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2612 - acc: 0.4967 - val_loss: 1.2943 - val_acc: 0.5000\n",
      "Epoch 30/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2478 - acc: 0.5113 - val_loss: 1.3092 - val_acc: 0.4878\n",
      "Epoch 31/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2447 - acc: 0.5098 - val_loss: 1.2951 - val_acc: 0.5000\n",
      "Epoch 32/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2384 - acc: 0.5110 - val_loss: 1.3855 - val_acc: 0.4634\n",
      "Epoch 33/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2435 - acc: 0.5086 - val_loss: 1.3260 - val_acc: 0.5000\n",
      "Epoch 34/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2315 - acc: 0.5102 - val_loss: 1.3326 - val_acc: 0.4634\n",
      "Epoch 35/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2332 - acc: 0.5133 - val_loss: 1.3723 - val_acc: 0.4390\n",
      "Epoch 36/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2158 - acc: 0.5192 - val_loss: 1.2711 - val_acc: 0.5732\n",
      "Epoch 37/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2229 - acc: 0.5137 - val_loss: 1.3551 - val_acc: 0.4634\n",
      "Epoch 38/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2103 - acc: 0.5228 - val_loss: 1.3200 - val_acc: 0.47561.2102 - acc:\n",
      "Epoch 39/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2029 - acc: 0.5213 - val_loss: 1.2818 - val_acc: 0.4878\n",
      "Epoch 40/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.2001 - acc: 0.5266 - val_loss: 1.3147 - val_acc: 0.5122\n",
      "Epoch 41/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1940 - acc: 0.5299 - val_loss: 1.3539 - val_acc: 0.4878\n",
      "Epoch 42/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1980 - acc: 0.5240 - val_loss: 1.2924 - val_acc: 0.4756\n",
      "Epoch 43/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1907 - acc: 0.5312 - val_loss: 1.3879 - val_acc: 0.4634\n",
      "Epoch 44/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1846 - acc: 0.5334 - val_loss: 1.3237 - val_acc: 0.4634\n",
      "Epoch 45/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1717 - acc: 0.5378 - val_loss: 1.3542 - val_acc: 0.5122\n",
      "Epoch 46/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1787 - acc: 0.5368 - val_loss: 1.3711 - val_acc: 0.4756\n",
      "Epoch 47/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1642 - acc: 0.5446 - val_loss: 1.2453 - val_acc: 0.5244\n",
      "Epoch 48/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1867 - acc: 0.5364 - val_loss: 1.3005 - val_acc: 0.5000\n",
      "Epoch 49/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1934 - acc: 0.5341 - val_loss: 1.3165 - val_acc: 0.5000\n",
      "Epoch 50/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1675 - acc: 0.5440 - val_loss: 1.3392 - val_acc: 0.4512\n",
      "Epoch 51/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1593 - acc: 0.5481 - val_loss: 1.2918 - val_acc: 0.4878\n",
      "Epoch 52/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1490 - acc: 0.5548 - val_loss: 1.4122 - val_acc: 0.4634\n",
      "Epoch 53/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1509 - acc: 0.5575 - val_loss: 1.4235 - val_acc: 0.4634\n",
      "Epoch 54/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1583 - acc: 0.5431 - val_loss: 1.4148 - val_acc: 0.4634\n",
      "Epoch 55/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1443 - acc: 0.5542 - val_loss: 1.2997 - val_acc: 0.5122\n",
      "Epoch 56/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1389 - acc: 0.5590 - val_loss: 1.2767 - val_acc: 0.5244\n",
      "Epoch 57/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1423 - acc: 0.5550 - val_loss: 1.2598 - val_acc: 0.5488\n",
      "Epoch 58/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1358 - acc: 0.5572 - val_loss: 1.3133 - val_acc: 0.4878\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1320 - acc: 0.5526 - val_loss: 1.2198 - val_acc: 0.5244\n",
      "Epoch 60/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1207 - acc: 0.5671 - val_loss: 1.1999 - val_acc: 0.5732\n",
      "Epoch 61/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1294 - acc: 0.5534 - val_loss: 1.3500 - val_acc: 0.4756\n",
      "Epoch 62/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1208 - acc: 0.5693 - val_loss: 1.3183 - val_acc: 0.4878\n",
      "Epoch 63/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1122 - acc: 0.5721 - val_loss: 1.2480 - val_acc: 0.5000\n",
      "Epoch 64/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1107 - acc: 0.5666 - val_loss: 1.3303 - val_acc: 0.4756\n",
      "Epoch 65/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1037 - acc: 0.5715 - val_loss: 1.2725 - val_acc: 0.4878\n",
      "Epoch 66/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1065 - acc: 0.5727 - val_loss: 1.2815 - val_acc: 0.4756\n",
      "Epoch 67/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1078 - acc: 0.5694 - val_loss: 1.2463 - val_acc: 0.5122\n",
      "Epoch 68/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0985 - acc: 0.5738 - val_loss: 1.3192 - val_acc: 0.4512\n",
      "Epoch 69/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0992 - acc: 0.5735 - val_loss: 1.2498 - val_acc: 0.5366\n",
      "Epoch 70/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1006 - acc: 0.5715 - val_loss: 1.3007 - val_acc: 0.4878\n",
      "Epoch 71/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.1029 - acc: 0.5705 - val_loss: 1.2611 - val_acc: 0.5488\n",
      "Epoch 72/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0901 - acc: 0.5791 - val_loss: 1.3404 - val_acc: 0.4512\n",
      "Epoch 73/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0889 - acc: 0.5774 - val_loss: 1.3526 - val_acc: 0.4878\n",
      "Epoch 74/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0960 - acc: 0.5754 - val_loss: 1.2610 - val_acc: 0.5122\n",
      "Epoch 75/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0913 - acc: 0.5755 - val_loss: 1.4166 - val_acc: 0.4512\n",
      "Epoch 76/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0816 - acc: 0.5783 - val_loss: 1.4073 - val_acc: 0.4390\n",
      "Epoch 77/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0764 - acc: 0.5834 - val_loss: 1.3170 - val_acc: 0.5000\n",
      "Epoch 78/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0872 - acc: 0.5793 - val_loss: 1.4297 - val_acc: 0.4146\n",
      "Epoch 79/150\n",
      "7360/7360 [==============================] - ETA: 0s - loss: 1.0727 - acc: 0.585 - 15s 2ms/step - loss: 1.0720 - acc: 0.5855 - val_loss: 1.3391 - val_acc: 0.4634\n",
      "Epoch 80/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0675 - acc: 0.5929 - val_loss: 1.3376 - val_acc: 0.4878\n",
      "Epoch 81/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0665 - acc: 0.5802 - val_loss: 1.4036 - val_acc: 0.4634\n",
      "Epoch 82/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0750 - acc: 0.5789 - val_loss: 1.2209 - val_acc: 0.5366\n",
      "Epoch 83/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0765 - acc: 0.5833 - val_loss: 1.3067 - val_acc: 0.4756\n",
      "Epoch 84/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0750 - acc: 0.5865 - val_loss: 1.3303 - val_acc: 0.4634\n",
      "Epoch 85/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0585 - acc: 0.5917 - val_loss: 1.4289 - val_acc: 0.4878\n",
      "Epoch 86/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0678 - acc: 0.5838 - val_loss: 1.3954 - val_acc: 0.4756\n",
      "Epoch 87/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0637 - acc: 0.5920 - val_loss: 1.4483 - val_acc: 0.4146\n",
      "Epoch 88/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0782 - acc: 0.5821 - val_loss: 1.2990 - val_acc: 0.5244\n",
      "Epoch 89/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0669 - acc: 0.5886 - val_loss: 1.3941 - val_acc: 0.4512\n",
      "Epoch 90/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0551 - acc: 0.5899 - val_loss: 1.2468 - val_acc: 0.5244\n",
      "Epoch 91/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0513 - acc: 0.5906 - val_loss: 1.2983 - val_acc: 0.5000\n",
      "Epoch 92/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0647 - acc: 0.5860 - val_loss: 1.2320 - val_acc: 0.5122\n",
      "Epoch 93/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0598 - acc: 0.5910 - val_loss: 1.2137 - val_acc: 0.5366\n",
      "Epoch 94/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0472 - acc: 0.5962 - val_loss: 1.3726 - val_acc: 0.5000\n",
      "Epoch 95/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0584 - acc: 0.5920 - val_loss: 1.2424 - val_acc: 0.5732\n",
      "Epoch 96/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0599 - acc: 0.5901 - val_loss: 1.2706 - val_acc: 0.5000\n",
      "Epoch 97/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0573 - acc: 0.5917 - val_loss: 1.1715 - val_acc: 0.5610\n",
      "Epoch 98/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0594 - acc: 0.5910 - val_loss: 1.1811 - val_acc: 0.5244\n",
      "Epoch 99/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0541 - acc: 0.5943 - val_loss: 1.4164 - val_acc: 0.4878\n",
      "Epoch 100/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0523 - acc: 0.5965 - val_loss: 1.3086 - val_acc: 0.5122\n",
      "Epoch 101/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0456 - acc: 0.6033 - val_loss: 1.3274 - val_acc: 0.4512.\n",
      "Epoch 102/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0428 - acc: 0.5938 - val_loss: 1.1848 - val_acc: 0.5244\n",
      "Epoch 103/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0555 - acc: 0.5902 - val_loss: 1.2544 - val_acc: 0.5122\n",
      "Epoch 104/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0409 - acc: 0.5997 - val_loss: 1.2481 - val_acc: 0.5244\n",
      "Epoch 105/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0441 - acc: 0.6054 - val_loss: 1.3128 - val_acc: 0.5122\n",
      "Epoch 106/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0557 - acc: 0.5905 - val_loss: 1.3586 - val_acc: 0.4756\n",
      "Epoch 107/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0354 - acc: 0.6015 - val_loss: 1.3183 - val_acc: 0.4756\n",
      "Epoch 108/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0398 - acc: 0.5982 - val_loss: 1.3405 - val_acc: 0.4878\n",
      "Epoch 109/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0301 - acc: 0.6038 - val_loss: 1.3210 - val_acc: 0.5000\n",
      "Epoch 110/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0397 - acc: 0.5948 - val_loss: 1.3116 - val_acc: 0.4878\n",
      "Epoch 111/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0273 - acc: 0.6030 - val_loss: 1.2315 - val_acc: 0.4756\n",
      "Epoch 112/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0311 - acc: 0.5982 - val_loss: 1.2509 - val_acc: 0.5244\n",
      "Epoch 113/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0288 - acc: 0.6019 - val_loss: 1.1848 - val_acc: 0.5244\n",
      "Epoch 114/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0261 - acc: 0.6026 - val_loss: 1.2926 - val_acc: 0.4756\n",
      "Epoch 115/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0267 - acc: 0.6037 - val_loss: 1.3803 - val_acc: 0.4756\n",
      "Epoch 116/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0174 - acc: 0.6155 - val_loss: 1.3279 - val_acc: 0.5122\n",
      "Epoch 117/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0222 - acc: 0.6073 - val_loss: 1.2511 - val_acc: 0.4878\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0285 - acc: 0.5989 - val_loss: 1.2905 - val_acc: 0.5122\n",
      "Epoch 119/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0229 - acc: 0.6098 - val_loss: 1.2787 - val_acc: 0.4756\n",
      "Epoch 120/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0288 - acc: 0.5992 - val_loss: 1.3344 - val_acc: 0.4756\n",
      "Epoch 121/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0253 - acc: 0.6058 - val_loss: 1.2747 - val_acc: 0.4634\n",
      "Epoch 122/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0203 - acc: 0.6060 - val_loss: 1.3654 - val_acc: 0.4878\n",
      "Epoch 123/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0139 - acc: 0.6061 - val_loss: 1.2170 - val_acc: 0.5122\n",
      "Epoch 124/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0084 - acc: 0.6122 - val_loss: 1.2299 - val_acc: 0.5000\n",
      "Epoch 125/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0116 - acc: 0.6155 - val_loss: 1.2954 - val_acc: 0.4878\n",
      "Epoch 126/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0074 - acc: 0.6143 - val_loss: 1.2439 - val_acc: 0.4634\n",
      "Epoch 127/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0056 - acc: 0.6193 - val_loss: 1.2892 - val_acc: 0.4878\n",
      "Epoch 128/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0076 - acc: 0.6190 - val_loss: 1.2111 - val_acc: 0.5122\n",
      "Epoch 129/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0128 - acc: 0.6098 - val_loss: 1.2367 - val_acc: 0.5000\n",
      "Epoch 130/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0011 - acc: 0.6188 - val_loss: 1.3068 - val_acc: 0.5122\n",
      "Epoch 131/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0111 - acc: 0.6099 - val_loss: 1.2772 - val_acc: 0.5244\n",
      "Epoch 132/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9941 - acc: 0.6143 - val_loss: 1.2478 - val_acc: 0.5122\n",
      "Epoch 133/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0014 - acc: 0.6166 - val_loss: 1.2733 - val_acc: 0.5122\n",
      "Epoch 134/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0112 - acc: 0.6082 - val_loss: 1.2909 - val_acc: 0.5244\n",
      "Epoch 135/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9988 - acc: 0.6109 - val_loss: 1.3241 - val_acc: 0.4878\n",
      "Epoch 136/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0040 - acc: 0.6106 - val_loss: 1.3998 - val_acc: 0.4512\n",
      "Epoch 137/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9973 - acc: 0.6139 - val_loss: 1.3844 - val_acc: 0.4878\n",
      "Epoch 138/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9982 - acc: 0.6149 - val_loss: 1.2163 - val_acc: 0.5488\n",
      "Epoch 139/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 1.0006 - acc: 0.6164 - val_loss: 1.2922 - val_acc: 0.4634\n",
      "Epoch 140/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9936 - acc: 0.6171 - val_loss: 1.2677 - val_acc: 0.5000\n",
      "Epoch 141/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9969 - acc: 0.6167 - val_loss: 1.3533 - val_acc: 0.5244\n",
      "Epoch 142/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9868 - acc: 0.6197 - val_loss: 1.2869 - val_acc: 0.4878\n",
      "Epoch 143/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9901 - acc: 0.6192 - val_loss: 1.2230 - val_acc: 0.5366\n",
      "Epoch 144/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9869 - acc: 0.6247 - val_loss: 1.3168 - val_acc: 0.5244\n",
      "Epoch 145/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9846 - acc: 0.6258 - val_loss: 1.3922 - val_acc: 0.4878\n",
      "Epoch 146/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9863 - acc: 0.6227 - val_loss: 1.3584 - val_acc: 0.5122\n",
      "Epoch 147/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9917 - acc: 0.6183 - val_loss: 1.3898 - val_acc: 0.4268\n",
      "Epoch 148/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9910 - acc: 0.6167 - val_loss: 1.3865 - val_acc: 0.4756\n",
      "Epoch 149/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9909 - acc: 0.6170 - val_loss: 1.2835 - val_acc: 0.5244\n",
      "Epoch 150/150\n",
      "7360/7360 [==============================] - 15s 2ms/step - loss: 0.9827 - acc: 0.6230 - val_loss: 1.1175 - val_acc: 0.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djaym7\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer cu_dnnlstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 's0:0' shape=(?, 96) dtype=float32>, <tf.Tensor 'c0:0' shape=(?, 96) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_num_people=[]\n",
    "for i in range(1,92):\n",
    "    if i <10:\n",
    "        total_num_people.append('100'+str(i))\n",
    "    else:\n",
    "        total_num_people.append('10'+str(i))\n",
    "\n",
    "for i in total_num_people:\n",
    "    if int(i)==1011:\n",
    "        X_train=[]\n",
    "        X_test=[]\n",
    "        y_train=[]\n",
    "        y_test=[]\n",
    "        X_train,y_train,X_test,y_test=loadData(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "    #load data here\n",
    "    #input_shape = batch_size,time_steps,input_dim\n",
    "\n",
    "    #batch_size = X_train.shape[0]   #automatically taken no need to specify\n",
    "    time_steps = X_train.shape[1]\n",
    "    input_dim = X_train.shape[2]\n",
    "\n",
    "\n",
    "\n",
    "    hid_pre_acts = [16]\n",
    "    hid_post_acts = [96]\n",
    "    #dense1s = [16,32,64,96,128,256]\n",
    "    #dense2s = [16,32,64,96,128,256]\n",
    "    for hid_pre_act in hid_pre_acts:\n",
    "        for hid_post_act in hid_post_acts:\n",
    "            # Defined shared layers as global variables\n",
    "            repeator = RepeatVector(time_steps) #500\n",
    "            concatenator = Concatenate(axis=-1)\n",
    "            densor = Dense(1, activation = \"relu\")\n",
    "            activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "            dotor = Dot(axes = 1)\n",
    "\n",
    "            #one step attention\n",
    "\n",
    "            def one_step_attention(a,s_prev):\n",
    "\n",
    "                s_prev = repeator(s_prev)\n",
    "                concat = concatenator([a,s_prev])\n",
    "                e = densor(concat)\n",
    "                alphas = activator(e)\n",
    "                context = dotor([alphas, a])\n",
    "\n",
    "                return context\n",
    "            #for dense1 in dense1s:\n",
    "                #for dense2 in dense2s:\n",
    "            Name = f'FINAL-[16,96] id -{i}---{int(time.time())}'\n",
    "            tensorboard = TensorBoard(log_dir=f'C:\\\\Users\\\\djaym7\\\\Desktop\\\\Github\\\\EmotionRecognition\\\\logs\\\\{Name}')\n",
    "            print(Name)\n",
    "            post_activation_lstm_cell = CuDNNLSTM(hid_post_act,return_state=True)\n",
    "            #output_layer = Dense(dense1,activation='relu')\n",
    "            #onemore = Dense(dense2,activation='relu')\n",
    "            fin_out = Dense(6,activation='softmax')\n",
    "\n",
    "                # model\n",
    "\n",
    "            def model(time_steps,input_dim,hid_post_act,hid_pre_act):\n",
    "\n",
    "                X = Input(shape =(time_steps,input_dim))\n",
    "                s0 = Input(shape=(hid_post_act,),name='s0')\n",
    "                c0 = Input(shape=(hid_post_act,),name='c0')\n",
    "                s=s0\n",
    "                c=c0\n",
    "                outputs=[]\n",
    "\n",
    "\n",
    "\n",
    "                a = CuDNNLSTM(hid_pre_act,return_sequences=True)(X)\n",
    "                a = Dropout(0.4)(a)\n",
    "                #6 is final output size\n",
    "                for t in range(1):\n",
    "                    context =one_step_attention(a,s)\n",
    "                    s, _, c = post_activation_lstm_cell(context, initial_state=[s, c])\n",
    "                    #out = output_layer(s)\n",
    "                    #o = onemore(out)\n",
    "                    k=fin_out(s)\n",
    "                    outputs.append(k)\n",
    "\n",
    "\n",
    "                model = Model([X, s0, c0], outputs)\n",
    "\n",
    "                return model       \n",
    "\n",
    "            model = model(time_steps,input_dim,hid_post_act,hid_pre_act)\n",
    "\n",
    "            opt = Adam()\n",
    "            #opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "            s0 = np.zeros((X_train.shape[0], hid_post_act))\n",
    "            c0 = np.zeros((X_train.shape[0], hid_post_act))\n",
    "            s0_t = np.zeros((X_test.shape[0], hid_post_act))\n",
    "            c0_t = np.zeros((X_test.shape[0], hid_post_act))\n",
    "            print(f'id : {i}')\n",
    "            model.fit([X_train, s0, c0], y_train, epochs=150,validation_data=([X_test,s0_t,c0_t],y_test),callbacks=[tensorboard])\n",
    "            model.save(f'{i}.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
